{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e158cfe",
   "metadata": {},
   "source": [
    "# Transformers tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks  Transformers\n",
    "-Sentiment analysis\n",
    "-Name entity recognition (NER)\n",
    "-Question answering \n",
    "-Filling masked text\n",
    "-Summarization \n",
    "-Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef23a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119546b",
   "metadata": {},
   "source": [
    "#   Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e87bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a916de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978193640708923}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_pipeline =pipeline(\"sentiment-analysis\")\n",
    "senti_pipeline('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "615d401e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991856217384338}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " senti_pipeline(\"i am  sad \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43133c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998695254325867}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " senti_pipeline(\" i like you  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b308b60",
   "metadata": {},
   "source": [
    "#  Question Answering using, HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d966f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    " from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08e8517d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9027f6d79eb40b1927f6fd5d9aa8a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9761040210723877,\n",
       " 'start': 11,\n",
       " 'end': 26,\n",
       " 'answer': 'Otman ait mbark'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = pipeline('question-answering')\n",
    "nlp({\n",
    "    'question': 'What is my name ?',\n",
    "    'context': 'My name is Otman ait mbark , I work at HuggingFace'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline('question-answering')\n",
    "nlp({\n",
    "    'question': '?أين اسكن',\n",
    "    'context': 'اعيش في المغرب ,اسكن في مدينة ورزازات'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48580a",
   "metadata": {},
   "source": [
    "#  Name entity recognition  # NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b52ce3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b2fe52f90548c7b58a299b6ceacb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-ORG',\n",
       "  'score': 0.9988007,\n",
       "  'index': 11,\n",
       "  'word': 'Hu',\n",
       "  'start': 27,\n",
       "  'end': 29},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99271137,\n",
       "  'index': 12,\n",
       "  'word': '##gging',\n",
       "  'start': 29,\n",
       "  'end': 34},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9977949,\n",
       "  'index': 13,\n",
       "  'word': '##F',\n",
       "  'start': 34,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9956417,\n",
       "  'index': 14,\n",
       "  'word': '##ace',\n",
       "  'start': 35,\n",
       "  'end': 38}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = pipeline('ner')\n",
    "nlp('It is me, otman, I work at HuggingFace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8733bd",
   "metadata": {},
   "source": [
    "# Predicting Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "958433ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'I hope you enjoyed this video',\n",
       "  'score': 0.7073915600776672,\n",
       "  'token': 3776,\n",
       "  'token_str': ' enjoyed'},\n",
       " {'sequence': 'I hope you enjoy this video',\n",
       "  'score': 0.13673606514930725,\n",
       "  'token': 2254,\n",
       "  'token_str': ' enjoy'},\n",
       " {'sequence': 'I hope you liked this video',\n",
       "  'score': 0.1335308700799942,\n",
       "  'token': 6640,\n",
       "  'token_str': ' liked'},\n",
       " {'sequence': 'I hope you like this video',\n",
       "  'score': 0.005779130384325981,\n",
       "  'token': 101,\n",
       "  'token_str': ' like'},\n",
       " {'sequence': 'I hope you appreciated this video',\n",
       "  'score': 0.005615242291241884,\n",
       "  'token': 10874,\n",
       "  'token_str': ' appreciated'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = pipeline('fill-mask')\n",
    "nlp('I hope you <mask> this video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70d3f285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': ' آمل أن يعجبكة هذا الفيديو ',\n",
       "  'score': 0.3754940629005432,\n",
       "  'token': 38419,\n",
       "  'token_str': 'ة'},\n",
       " {'sequence': ' آمل أن يعجبكا هذا الفيديو ',\n",
       "  'score': 0.2691783308982849,\n",
       "  'token': 29438,\n",
       "  'token_str': 'ا'},\n",
       " {'sequence': ' آمل أن يعجبكل هذا الفيديو ',\n",
       "  'score': 0.07041101157665253,\n",
       "  'token': 30992,\n",
       "  'token_str': 'ل'},\n",
       " {'sequence': ' آمل أن يعجبكي هذا الفيديو ',\n",
       "  'score': 0.05075579509139061,\n",
       "  'token': 26174,\n",
       "  'token_str': 'ي'},\n",
       " {'sequence': ' آمل أن يعجبكن هذا الفيديو ',\n",
       "  'score': 0.039414819329977036,\n",
       "  'token': 31746,\n",
       "  'token_str': 'ن'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = pipeline('fill-mask')\n",
    "nlp(' آمل أن يعجبك<mask> هذا الفيديو ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53970ad1",
   "metadata": {},
   "source": [
    "# Single Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"let us see how this turns\"\n",
    "indexed_tokens = tokeniser.encode(text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "model.eval()\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e17808",
   "metadata": {},
   "source": [
    "# Summarising text using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2583c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text =\"\"\"Perhaps one of the most significant advances made by Arabic mathematics began at this time with the work of al-Khwarizmi, namely the beginnings of algebra. It is important to understand just how significant this new idea was. It was a revolutionary move away from the Greek concept of mathematics which was essentially geometry. Algebra was a unifying theory which allowedrational numbers,irrational numbers, geometrical magnitudes, etc., to all be treated as \\\"algebraic objects\\\". It gave mathematics a whole new development path so much broader in concept to that which had existed before, and provided a vehicle for future development of the subject. Another important aspect of the introduction of algebraic ideas was that it allowed mathematics to be applied to itselfin a way which had not happened before.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c344e2b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-7ea5b3f3fe14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# using pipeline API for summarization task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msummarization\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"summarization\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msummary_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummarization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'summary_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[1;31m# Will load the correct model if possible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[0mmodel_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m     framework, model = infer_framework_load_model(\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[0mmodel_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Could not load model {model} with any of the following classes: {class_tuple}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mframework\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"tf\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TF\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>)."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# using pipeline API for summarization task\n",
    "summarization = pipeline(\"summarization\")\n",
    "\n",
    "summary_text = summarization(original_text)[0]['summary_text']\n",
    "print(\"Summary:\", summary_text)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f657dff1",
   "metadata": {},
   "source": [
    "# English to German Translation¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf3a28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1b38d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d297c76f00431aa2fafb87c3e278d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation = pipeline(\"translation_en_to_de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c990a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I like to study Data Science and Machine Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9346440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich studiere gerne Datenwissenschaft und maschinelles Lernen\n"
     ]
    }
   ],
   "source": [
    "translated_text = translation(text, max_length=40)[0]['translation_text']\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00dded",
   "metadata": {},
   "source": [
    "# Translation (e.g. English to Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5648679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:898: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9adfe0f9954dac95520cc646ad3937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e377df84bee456f8849950b7404b7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdcf4435f8849318665a27b33275786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336e4f0464cc4fbba3823946410d60ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/806k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e6cefc553b431abc957e3291642cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/805k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af22de43fac04a52b0c6de56cc3ddcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "267198e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = pipeline(\"translation_en_to_zh\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37774688",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I like to study Data Science and Machine Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfb8be32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我喜欢学习数据科学和机器学习\n"
     ]
    }
   ],
   "source": [
    "translated_text = translation(text, max_length=40)[0]['translation_text']\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc73574",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c69cb68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, I create the model and it is not what I want as a programming language. It is what I use to\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I feel like I'm the only person with such a strong sense of language ability or such an understanding for\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so I can make sense of this syntax and do other things.\\n\\nIn languages like Swift and C#\"},\n",
       " {'generated_text': \"Hello, I'm a language model, a tool, a kind of humanist. In this case, I'm just a person who wants to give\"}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fcc4ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'bonjour,Je suis étudiant,Je se revève.\\n\\nJe suis étudiant et cette'},\n",
       " {'generated_text': \"bonjour,Je suis étudiant,Cœur j'avoir des sieux et là vous s\"},\n",
       " {'generated_text': 'bonjour,Je suis étudiant, sordid, qui se retiro sont présenterant toutes t'},\n",
       " {'generated_text': \"bonjour,Je suis étudiant, avec vous voirez mohi, et qu'avait ich\"}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"bonjour,Je suis étudiant,\", max_length=30, num_return_sequences=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce98d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
